Optimization refers to the task of either minimizing or maximizing some function of effects by altering X.  <br/>
* Gradient descent - Tries to find the minimum of loss function by altering weight values. 
* Momentum reduces learning rate when gradient values are small. AdaGrad gives frequently occurring features lower learning rates. AdaDelta improves AdaGrad by avoiding reducing learning rate to zero. Adam is basically AdaGrad with a bunch of fixes. Ftrl or follow the regularized leader works well on white models. At this time, Adam and ftrl make good defaults for deep neural networks as well as linear models. Who knows what new optimization techniques will become mainstream tomorrow










