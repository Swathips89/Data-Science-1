Optimization refers to the task of either minimizing or maximizing some function of effects by altering X.  <br/>
* Gradient descent - Tries to find the minimum of loss function by altering weight values. 
* Momentum - Reduces learning rate when gradient values are small. 
* AdaGrad - Gives frequently occurring features lower learning rates. 
* AdaDelta - Improves AdaGrad by avoiding reducing learning rate to zero.
* Adam - AdaGrad with a bunch of fixes. 
* Ftrl or follow the regularized leader - Works well on wide models.
Adam and ftrl make good defaults for deep neural networks as well as linear models. 









