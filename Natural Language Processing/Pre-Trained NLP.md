8 Pre-trained models: https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/?utm_source=linkedin.com&utm_medium=social <br/>

Glove Pre-Trained https://nlp.stanford.edu/projects/glove/ <br/>

Language Models: BERT, GPT-2, Megatron-LM

Turing NLG: Natural Language Generation - Transformer based Generative Language Model - 17 billion parameters
* Taks: Question Answering, Document Summarizing/understanding, Conversational agents, Digital assistants
* Python: DeepSpeed Lib and ZeRO optimizer, PyTorch. Consists of 78 Transformer layers with a hidden size of 4256 and 28 attention heads







