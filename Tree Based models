A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better
than random guessing). In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification

Boosting algorithm always considers weak learners in order to prevent overfitting, since the complexity of the overall learner
increases at each step. Starting with weak learners implies the final classifier will be less likely to overfit

Each weak learner has a high variance, low bias

Bagging consists of sampling rows and variables randomly - so each individual tree is independent of the other. 
Result of many weaklearners is combined to create a strong learner

Both options are true. In Bagging, each individual trees are independent of each other because they consider different subset of features 
and samples

In boosting tree individual weak learners are not independent of each other because each tree correct the results of previous tree. 
Bagging and boosting both can be consider as improving the base learners results

Random Forest and Gradient Boosting ensemble methods are designed for classification as well as regression task

Random forest is based on bagging concept, that consider faction of sample and faction of feature for building the individual trees

Increase the depth from the certain value of depth may overfit the data and for 2 depth values validation accuracies are same we always 
prefer the small depth in final model building

Bagging doesnt have 'learning rate' parameter 

Model that gives largest AUC is preferred

Since Random Forest aggregate the result of different weak learners, If It is possible we would want more number of trees in model building.
Random Forest is a black box model you will lose interpretability after using it.

In RandomForest, if n tress contributing to the RandomForest all have an accuracy of 70%, min accuracy of overall RandomForest 
can be from less than 70% to 100%

GBM can use gradient decent method for minimize the loss function. 

Time to train GBM model increases with increase in no of trees, whereas learning rate doesnt effect time to train 

Bagging is suitable for high variance low bias models

Learning rate should be low but it should not be very low otherwise algorithm will take so long to finish the training because you need 
to increase the number trees

How to select best hyperparameters in tree based models?


