# Bagging consists of sampling rows and variables randomly - so each individual tree is independent of the other. 
Result of many weaklearners is combined to create a strong learner

Both options are true. In Bagging, each individual trees are independent of each other because they consider different subset of features 
and samples

In boosting tree individual weak learners are not independent of each other because each tree correct the results of previous tree. 
Bagging and boosting both can be consider as improving the base learners results

Random Forest and Gradient Boosting ensemble methods are designed for classification as well as regression task

Random forest is based on bagging concept, that consider faction of sample and faction of feature for building the individual trees

Increase the depth from the certain value of depth may overfit the data and for 2 depth values validation accuracies are same we always 
prefer the small depth in final model building

Bagging doesnt have 'learning rate' parameter 

Model that gives largest AUC is preferred

Since Random Forest aggregate the result of different weak learners, If It is possible we would want more number of trees in model building.
Random Forest is a black box model you will lose interpretability after using it.




