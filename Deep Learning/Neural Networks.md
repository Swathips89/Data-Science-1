

A neuron will take an input, apply some activation function to it, and generate an output. Rectified Linear Unit (ReLU) is one of the most commonly used activation function.

![](https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/10/Screenshot-from-2018-10-12-14-29-37-850x438.png)
As the amount of data increases, the performance of traditional learning algorithms, like SVM and logistic regression, does not improve by a whole lot. In fact, it tends to plateau after a certain point. In the case of neural networks, the performance of the model increases with an increase in the data you feed to the model.
There are basically three scales that drive a typical deep learning process:
1. Data
2. Computation Time
3. Algorithms











